## ETL Process Documentation

### Overview
The purpose of this document is to provide the methodology and rationale for our approach to creating and loading sample data into the Dream Homes NYC database. The guiding principle for the ETL process was to use SQL whenever possible to reduce potential issues with conflicting dependencies when loading the data across test systems and in production.

Despite the plan to use SQL as much as possible, using Python for the actual generation of test data made the most sense. However, once the data was created and exported to CSV, SQL will be used to perform all of the remaining steps from import, transformation, and loading to the final schema.

### Step 1
#### Creating and Importing Sample Data

##### Approach
- We utilized generative AI (ChatGPT-4o) to create the scripts to produce the sample data based on our defined schema. While it was helpful in some cases, quite a bit of rework was needed to get the final code.
- Python was used to generate the sample data, and export the data to denormalized CSV files.
- Due to complexities of file paths and operating systems, we opted to load the CSV data into our database using the psycopg2 package rather than using the COPY function in PostgreSQL, which was our original plan. This method was no more complicated, and had the benefit of being platform agnostic.
- No id's were generated for the CSV outputs with the intent to allow our schema to auto-generate all primary and foreign keys through SQL.

##### Consolidating Tables for Data Generation
- Denormalized **offices** include address and office details in a single **offices.csv** data file.
- Denormalized **clients** include the person, address, and client details in a single **clients.csv** data file.
  - Up to five interactions are added to client_interactions table for each client record via **client_interactions.csv** data file.
- Denormalized **employees** include the person, office, address, employee, and agent details in a single **employees.csv** data file.
- Denormalized **properties** include the address, property details, and listing agent in a single **properties.csv** data file.
  - Up to five reviews are added to **property_reviews** table for each property record via **property_reviews.csv** data file.
  - Up to ten images are added to **property_images** table for each property record via **property_images.csv** data file.
- Denormalized **events** include the property, client, agent, and event details in a single **events.csv** data file.
- Denormalized **transactions** include the property, client, agent, and transaction details in a single **transactions.csv** data file.
- Denormalized **business_expenses** include the office and expense details in a single **business_expenses.csv** data file.

##### Other Tables
- All reference tables, such as property_types and client_types, are generated from randomly assigned values included in the denormalized files.

##### Special Considerations
- The text and values generated by the faker package in Python were all but useless except as field fillers, and did not reflect the standard of sample data we required. Custom providers were generated to create more relevant values for addresses, property descriptions, amenities, as well as employee and client names. Whenever we found the data unusable, or simply unattractive, a custom provider was utilized to meet our needs.

### Step 2
#### Loading Sample Data to Schema

##### Approach
Each of the denormalized CSV files was created with enough details of any related tables to allow for a lookup to determine the proper foreign key to use when inserting the data to the schema. When loading the tables, we used temporary tables created from the CSV exports in the earlier phase of the project. Those tables were then queried to pull the data to be inserted, and joined on related tables to get proper keys to allow insertion.

All of the data was loaded to the schema from these temporary tables, and some final checks on expected record counts were added to the import script to validate the process. Should more data be required, the python scripts can be easily updated to draw more records for any given table.