{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "from custom_providers import NYCAddressProvider, NYCPersonProvider, RentalDescriptionProvider, PropertyAmenitiesProvider, ExtendedZipcodeProvider\n",
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the custom providers for more relevant sample data, and where the data files will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = Faker()\n",
    "\n",
    "# configure the custom providers for our fake data elements\n",
    "fake.add_provider(NYCAddressProvider)\n",
    "fake.add_provider(NYCPersonProvider)\n",
    "fake.add_provider(RentalDescriptionProvider)\n",
    "fake.add_provider(PropertyAmenitiesProvider)\n",
    "fake.add_provider(ExtendedZipcodeProvider)\n",
    "\n",
    "# Create the data subdirectory if it doesn't exist\n",
    "subdirectory = 'data'\n",
    "os.makedirs(subdirectory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 10 offices for the organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of New York areas\n",
    "nyc_areas = [\n",
    "    'Manhattan', 'Brooklyn', 'Queens', 'Bronx', 'Staten Island', \n",
    "    'Harlem', 'Upper West Side', 'Chelsea', 'Greenwich Village', \n",
    "    'SoHo', 'Tribeca', 'Williamsburg', 'Long Island City', \n",
    "    'Astoria', 'Flushing'\n",
    "]\n",
    "\n",
    "# Ensure we only use unique names from the list\n",
    "nyc_areas = nyc_areas[:10]\n",
    "\n",
    "offices_data = []\n",
    "\n",
    "for i in range(len(nyc_areas)):\n",
    "    address = fake.nyc_address()\n",
    "    office_record = {\n",
    "        'office_name': f'Dream Homes NYC {nyc_areas[i]} Branch',\n",
    "        'address': address['address'],\n",
    "        'city': address['city'],\n",
    "        'state': address['state'],\n",
    "        'zipcode': address['zipcode'],\n",
    "        'neighborhood': address['neighborhood'],\n",
    "        'phone_number': fake.phone_number(),\n",
    "        'email': fake.company_email()\n",
    "    }\n",
    "    offices_data.append(office_record)\n",
    "\n",
    "offices_df = pd.DataFrame(offices_data)\n",
    "offices_csv = os.path.join(subdirectory, 'offices.csv')\n",
    "offices_df.to_csv(offices_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 250 denormalized employee records, and designate 80% as agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_data = []\n",
    "agents_data = []\n",
    "used_names = set()\n",
    "specializations = ['Residential', 'Commercial', 'Luxury', 'Investment', 'Leasing', 'Property Management', 'Foreclosures', 'Short Sales']\n",
    "\n",
    "# Define the desired employment status distribution\n",
    "employment_status_distribution = {\n",
    "    'Active': 0.7,\n",
    "    'Inactive': 0.1,\n",
    "    'On Leave': 0.1,\n",
    "    'Terminated': 0.1\n",
    "}\n",
    "\n",
    "employment_status_choices = (\n",
    "    ['Active'] * int(employment_status_distribution['Active'] * 100) +\n",
    "    ['Inactive'] * int(employment_status_distribution['Inactive'] * 100) +\n",
    "    ['On Leave'] * int(employment_status_distribution['On Leave'] * 100) +\n",
    "    ['Terminated'] * int(employment_status_distribution['Terminated'] * 100)\n",
    ")\n",
    "\n",
    "for i in range(250):\n",
    "    address = fake.nyc_address()\n",
    "    person = fake.nyc_person(used_names)\n",
    "    office = random.choice(offices_data)  # Select a random office for the employee\n",
    "    employment_status = random.choice(employment_status_choices)\n",
    "    termination_date = fake.date_between(start_date='-5y', end_date=date(2024, 6, 30)) if employment_status == 'Terminated' else None\n",
    "    specialties = ', '.join(random.sample(specializations, random.randint(1, 5))) if i % 2 == 0 else None\n",
    "    \n",
    "    employee_record = {\n",
    "        'name': person['name'],\n",
    "        'email': person['email'],\n",
    "        'phone_number': person['phone_number'],\n",
    "        'date_of_birth': fake.date_of_birth(minimum_age=18, maximum_age=80),\n",
    "        'address': address['address'],\n",
    "        'city': address['city'],\n",
    "        'state': address['state'],\n",
    "        'zipcode': address['zipcode'],\n",
    "        'neighborhood': address['neighborhood'],\n",
    "        'office_phone_number': office['phone_number'],\n",
    "        'office_email': office['email'],\n",
    "        'employment_status': employment_status,\n",
    "        'sales_total': 0,\n",
    "        'hire_date': fake.date_between(start_date='-5y', end_date=date(2024, 6, 30)),\n",
    "        'termination_date': termination_date,\n",
    "        'license_number': fake.bothify(text='???-####-####') if i % 2 == 0 else None,\n",
    "        'specialties': specialties,\n",
    "        'rating': round(random.uniform(1, 5), 2) if i % 2 == 0 else None\n",
    "    }\n",
    "    employees_data.append(employee_record.copy())\n",
    "\n",
    "    if random.random() < 0.8:\n",
    "        agents_data.append(employee_record.copy())\n",
    "\n",
    "employees_df = pd.DataFrame(employees_data)\n",
    "employees_csv = os.path.join(subdirectory, 'employees.csv')\n",
    "employees_df.to_csv(employees_csv, index=False)\n",
    "\n",
    "agents_df = pd.DataFrame(agents_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 1000 denormalized client records, and up to five interactions with agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static Property Types and Statuses\n",
    "client_types = ['Corporate', 'Individual', 'Non-Profit', 'Government', 'Small Business', 'VIP']\n",
    "interaction_types = ['Email', 'Phone Call', 'In-Person Meeting', 'Follow-Up', 'Inquiry']\n",
    "\n",
    "clients_data = []\n",
    "client_interactions_data = []\n",
    "used_names = set()\n",
    "\n",
    "for _ in range(1000):\n",
    "    address = fake.nyc_address()\n",
    "    person = fake.nyc_person(used_names)\n",
    "    agent = random.choice(agents_data)  # Select a random agent for any interactions\n",
    "    client_record = {\n",
    "        'name': person['name'],\n",
    "        'email': person['email'],\n",
    "        'phone_number': person['phone_number'],\n",
    "        'date_of_birth': fake.date_of_birth(minimum_age=18, maximum_age=80),\n",
    "        'address': address['address'],\n",
    "        'city': address['city'],\n",
    "        'state': address['state'],\n",
    "        'zipcode': address['zipcode'],\n",
    "        'neighborhood': address['neighborhood'],\n",
    "        'preferred_contact_method': random.choice(['Email', 'Phone', 'Mail']),\n",
    "        'notes': fake.text(),\n",
    "        'client_type': random.choice(client_types)\n",
    "    }\n",
    "    clients_data.append(client_record)\n",
    "\n",
    "    num_interactions = random.randint(0, 5)\n",
    "    for _ in range(num_interactions):\n",
    "        interaction_record = {\n",
    "            'client_name': client_record['name'],\n",
    "            'client_email': client_record['email'],\n",
    "            'client_date_of_birth': client_record['date_of_birth'],\n",
    "            'agent_name': agent['name'],\n",
    "            'agent_email': agent['email'],\n",
    "            'agent_date_of_birth': agent['date_of_birth'],\n",
    "            'interaction_date': fake.date_between(start_date='-5y', end_date=date(2024, 6, 30)),\n",
    "            'interaction_type': random.choice(interaction_types),\n",
    "            'notes': fake.text()\n",
    "        }\n",
    "        client_interactions_data.append(interaction_record)\n",
    "\n",
    "clients_df = pd.DataFrame(clients_data)\n",
    "clients_csv = os.path.join(subdirectory, 'clients.csv')\n",
    "clients_df.to_csv(clients_csv, index=False)\n",
    "\n",
    "client_interactions_df = pd.DataFrame(client_interactions_data)\n",
    "client_interactions_csv = os.path.join(subdirectory, 'client_interactions.csv')\n",
    "client_interactions_df.to_csv(client_interactions_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 20000 denormalized property records, randomize the property_type, property_status, and listing_agent assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static Property Types and Statuses\n",
    "property_types = ['Apartment', 'Townhouse', 'Condo', 'Villa', 'Studio']\n",
    "property_statuses = ['Pending', 'Reserved', 'Sold', 'Listed', 'Unavailable']\n",
    "\n",
    "# Define realistic price ranges based on property type\n",
    "price_ranges = {\n",
    "    'Apartment': (200000, 500000),\n",
    "    'Townhouse': (300000, 800000),\n",
    "    'Condo': (250000, 600000),\n",
    "    'Villa': (600000, 1500000),\n",
    "    'Studio': (150000, 400000)\n",
    "}\n",
    "\n",
    "# Define realistic rental price ranges based on property type\n",
    "rental_price_ranges = {\n",
    "    'Apartment': (1500, 4000),\n",
    "    'Townhouse': (2500, 7000),\n",
    "    'Condo': (1800, 5000),\n",
    "    'Villa': (4000, 12000),\n",
    "    'Studio': (1000, 3000)\n",
    "}\n",
    "\n",
    "properties_data = []\n",
    "property_reviews_data = []\n",
    "property_images_data = []\n",
    "\n",
    "for _ in range(20000):\n",
    "    address = fake.nyc_address()\n",
    "    agent = random.choice(agents_data)  # Select a random agent for the listing agent\n",
    "    \n",
    "    # Determine listing type (70% rentals, 30% sales)\n",
    "    if random.random() < 0.7:\n",
    "        listing_type = 'rent'\n",
    "        property_type = random.choice(property_types)\n",
    "        min_price, max_price = rental_price_ranges[property_type]\n",
    "    else:\n",
    "        listing_type = 'sale'\n",
    "        property_type = random.choice(property_types)\n",
    "        min_price, max_price = price_ranges[property_type]\n",
    "    \n",
    "    # Generate a property price within this range\n",
    "    property_price = round(random.uniform(min_price, max_price), 2)\n",
    "\n",
    "    # Determine property status with the desired distribution\n",
    "    status_probability = random.random()\n",
    "    if status_probability < 0.02:\n",
    "        status_name = 'Unavailable'\n",
    "    elif status_probability < 0.04:\n",
    "        status_name = 'Pending'\n",
    "    elif status_probability < 0.06:\n",
    "        status_name = 'Reserved'\n",
    "    elif status_probability < 0.36:\n",
    "        status_name = 'Listed'\n",
    "    else:\n",
    "        status_name = 'Sold'\n",
    "\n",
    "    # If the property status is 'Listed', ensure the listing date is within 300 days of the last day of the sample\n",
    "    if status_name == 'Listed':\n",
    "        listing_date = fake.date_between(start_date='-300d', end_date=date(2024, 6, 30))\n",
    "    else:\n",
    "        listing_date = fake.date_between(start_date='-5y', end_date=date(2024, 6, 30))\n",
    "\n",
    "    property_record = {\n",
    "        'address': address['address'],\n",
    "        'city': address['city'],\n",
    "        'state': address['state'],\n",
    "        'zipcode': address['zipcode'],\n",
    "        'neighborhood': address['neighborhood'],\n",
    "        'type_name': property_type,\n",
    "        'status_name': status_name,\n",
    "        'price': property_price,\n",
    "        'square_feet': random.randint(500, 5000),\n",
    "        'number_of_bedrooms': random.randint(1, 5),\n",
    "        'number_of_bathrooms': random.randint(1, 3),\n",
    "        'year_built': random.randint(1900, 2021),\n",
    "        'description': fake.rental_description(),\n",
    "        'amenities': fake.property_amenities(),\n",
    "        'listing_date': listing_date,\n",
    "        'agent_name': agent['name'],\n",
    "        'agent_email': agent['email'],\n",
    "        'agent_phone': agent['phone_number'],\n",
    "        'agent_date_of_birth': agent['date_of_birth'],\n",
    "        'listing_type': listing_type  # Add listing type (rent or sale)\n",
    "    }\n",
    "    properties_data.append(property_record)\n",
    "\n",
    "    # Generate up to 5 property reviews\n",
    "    num_reviews = random.randint(0, 5)\n",
    "    for _ in range(num_reviews):\n",
    "        client = random.choice(clients_data)\n",
    "        review_record = {\n",
    "            'property_address': address['address'],\n",
    "            'property_city': address['city'],\n",
    "            'property_state': address['state'],\n",
    "            'property_zipcode': address['zipcode'],\n",
    "            'client_name': client['name'],\n",
    "            'client_email': client['email'],\n",
    "            'client_date_of_birth': client['date_of_birth'],\n",
    "            'review_date': fake.date_between(start_date='-5y', end_date=date(2024, 6, 30)),\n",
    "            'rating': random.randint(1, 5),\n",
    "            'review_text': fake.text()\n",
    "        }\n",
    "        property_reviews_data.append(review_record)\n",
    "\n",
    "    # Generate up to 10 property images\n",
    "    num_images = random.randint(0, 10)\n",
    "    for _ in range(num_images):\n",
    "        image_record = {\n",
    "            'property_address': address['address'],\n",
    "            'property_city': address['city'],\n",
    "            'property_state': address['state'],\n",
    "            'property_zipcode': address['zipcode'],\n",
    "            'image_url': fake.image_url(),\n",
    "            'description': fake.text(),\n",
    "            'uploaded_date': fake.date_between(start_date='-5y', end_date=date(2024, 6, 30))\n",
    "        }\n",
    "        property_images_data.append(image_record)\n",
    "\n",
    "# Save the dataframes to CSV files\n",
    "properties_df = pd.DataFrame(properties_data)\n",
    "properties_csv = os.path.join(subdirectory, 'properties.csv')\n",
    "properties_df.to_csv(properties_csv, index=False)\n",
    "\n",
    "property_reviews_df = pd.DataFrame(property_reviews_data)\n",
    "property_reviews_csv = os.path.join(subdirectory, 'property_reviews.csv')\n",
    "property_reviews_df.to_csv(property_reviews_csv, index=False)\n",
    "\n",
    "property_images_df = pd.DataFrame(property_images_data)\n",
    "property_images_csv = os.path.join(subdirectory, 'property_images.csv')\n",
    "property_images_df.to_csv(property_images_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 5000 denormalized event records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_types = [\n",
    "    'Personal Showing', 'Virtual Tour', 'Open House'\n",
    "]\n",
    "\n",
    "# Generate Events\n",
    "events_data = []\n",
    "for _ in range(5000):\n",
    "    agent = random.choice(agents_data)          # Select a random agent for the event\n",
    "    property = random.choice(properties_data)   # Select a random property for the event\n",
    "    client = random.choice(clients_data)        # Select a random client for the event\n",
    "    event_record = {\n",
    "        'type_name': random.choice(event_types),\n",
    "        'date': fake.date_between(start_date='-5y', end_date=date(2024, 6, 30)),\n",
    "        'client_attended': random.choice([True, False]),\n",
    "        'duration': random.randint(1, 2) * 60,\n",
    "        'client_name': client['name'],\n",
    "        'client_email': client['email'],\n",
    "        'client_date_of_birth': client['date_of_birth'],\n",
    "        'property_address': property['address'],\n",
    "        'property_city': property['city'],\n",
    "        'property_state': property['state'],\n",
    "        'property_zipcode': property['zipcode'],\n",
    "        'property_neighborhood': property['neighborhood'],\n",
    "        'agent_name': agent['name'],\n",
    "        'agent_email': agent['email'],\n",
    "        'agent_phone': agent['phone_number'],\n",
    "        'agent_date_of_birth': agent['date_of_birth']\n",
    "    }\n",
    "    events_data.append(event_record)\n",
    "\n",
    "events_df = pd.DataFrame(events_data)\n",
    "events_csv = os.path.join(subdirectory, 'events.csv')\n",
    "events_df.to_csv(events_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 2000 denormalized transaction records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_types = ['Purchase', 'Sale', 'Rent']\n",
    "\n",
    "transactions_data = []\n",
    "for _ in range(2000):\n",
    "    agent = random.choice(agents_data)\n",
    "    client = random.choice(clients_data)\n",
    "    property = random.choice(properties_data)\n",
    "    transaction_type = random.choice(transaction_types)\n",
    "\n",
    "    # Ensure the transaction price is within 3% of the property's price\n",
    "    property_price = property['price']\n",
    "    min_transaction_price = property_price * 0.97  # 3% below the property price\n",
    "    max_transaction_price = property_price * 1.03  # 3% above the property price\n",
    "    \n",
    "    # Generate a transaction price within this range\n",
    "    transaction_price = round(random.uniform(min_transaction_price, max_transaction_price), 2)\n",
    "    \n",
    "    # Calculate commission based on the listing type\n",
    "    if property['listing_type'] == 'Rent':\n",
    "        commission = transaction_price  # Commission equals the transaction price for rentals\n",
    "    else:\n",
    "        # Calculate commission as a random percentage of the price (3-5%) for sales\n",
    "        commission_percentage = random.uniform(3, 5)\n",
    "        commission = round((commission_percentage / 100) * transaction_price, 2)\n",
    "    \n",
    "    # Generate transaction dates\n",
    "    transaction_date = fake.date_between(start_date='-5y', end_date=date(2024, 6, 30))\n",
    "    contract_signed_date = fake.date_between(start_date=transaction_date, end_date=transaction_date + timedelta(days=60))\n",
    "    closing_date = fake.date_between(start_date=transaction_date, end_date=transaction_date + timedelta(days=60))\n",
    "\n",
    "    transaction_record = {\n",
    "        'property_address': property['address'],\n",
    "        'property_city': property['city'],\n",
    "        'property_state': property['state'],\n",
    "        'property_zipcode': property['zipcode'],\n",
    "        'property_neighborhood': property['neighborhood'],\n",
    "        'client_name': client['name'],\n",
    "        'client_email': client['email'],\n",
    "        'client_date_of_birth': client['date_of_birth'],\n",
    "        'agent_name': agent['name'],\n",
    "        'agent_email': agent['email'],\n",
    "        'agent_phone': agent['phone_number'],\n",
    "        'agent_date_of_birth': agent['date_of_birth'],\n",
    "        'transaction_type': transaction_type,\n",
    "        'price': transaction_price,\n",
    "        'commission': commission,\n",
    "        'date': transaction_date,\n",
    "        'contract_signed_date': contract_signed_date,\n",
    "        'closing_date': closing_date\n",
    "    }\n",
    "    transactions_data.append(transaction_record)\n",
    "\n",
    "transactions_df = pd.DataFrame(transactions_data)\n",
    "transactions_csv = os.path.join(subdirectory, 'transactions.csv')\n",
    "transactions_df.to_csv(transactions_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 1000 business expense records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "expense_types = ['Utilities', 'Marketing', 'Travel', 'Office Supplies', 'Maintenance', 'Technology', 'Training', 'Event', 'Miscellaneous']\n",
    "non_rent_expense_types = ['Utilities', 'Marketing', 'Travel', 'Office Supplies', 'Maintenance', 'Technology', 'Training', 'Event', 'Miscellaneous']\n",
    "\n",
    "# Custom distribution for non-rent expenses\n",
    "non_rent_distribution = {\n",
    "    'Utilities': 0.1,\n",
    "    'Marketing': 0.2,\n",
    "    'Travel': 0.1,\n",
    "    'Office Supplies': 0.1,\n",
    "    'Maintenance': 0.1,\n",
    "    'Technology': 0.1,\n",
    "    'Training': 0.1,\n",
    "    'Event': 0.1,\n",
    "    'Miscellaneous': 0.1\n",
    "}\n",
    "\n",
    "# Create a weighted list for non-rent expenses\n",
    "non_rent_expense_choices = []\n",
    "for expense_type, weight in non_rent_distribution.items():\n",
    "    non_rent_expense_choices.extend([expense_type] * int(weight * 100))\n",
    "\n",
    "business_expenses_data = []\n",
    "monthly_expenses = defaultdict(float)\n",
    "\n",
    "# Assign consistent rent amounts to each office\n",
    "office_rent_amounts = {\n",
    "    office['office_name']: round(random.uniform(3000, 5000), 2)\n",
    "    for office in offices_data\n",
    "}\n",
    "\n",
    "# Generate rent expenses for each office with a randomly selected start date\n",
    "current_date = date(2024, 6, 30)\n",
    "for office in offices_data:\n",
    "    rent_amount = office_rent_amounts[office['office_name']]\n",
    "    start_date = fake.date_between(start_date='-5y', end_date=date(2024, 6, 30))\n",
    "    employee = random.choice(employees_data)\n",
    "    \n",
    "    rent_date = start_date\n",
    "    while rent_date <= current_date:\n",
    "        if monthly_expenses[rent_date.strftime('%Y-%m')] + rent_amount > 10000:\n",
    "            rent_date += relativedelta(months=1)\n",
    "            continue\n",
    "        \n",
    "        expense_record = {\n",
    "            'office_name': office['office_name'],\n",
    "            'office_address': office['address'],\n",
    "            'office_city': office['city'],\n",
    "            'office_state': office['state'],\n",
    "            'office_zipcode': office['zipcode'],\n",
    "            'expense_date': rent_date,\n",
    "            'expense_type': 'Rent',\n",
    "            'amount': rent_amount,\n",
    "            'description': '',\n",
    "            'approved_by_name': employee['name'],\n",
    "            'approved_by_email': employee['email'],\n",
    "            'approved_by_phone_number': employee['phone_number']\n",
    "        }\n",
    "        business_expenses_data.append(expense_record)\n",
    "        monthly_expenses[rent_date.strftime('%Y-%m')] += rent_amount\n",
    "        \n",
    "        rent_date += relativedelta(months=1)\n",
    "\n",
    "# Generate 1000 records for other expenses\n",
    "for _ in range(1000):\n",
    "    office = random.choice(offices_data)\n",
    "    employee = random.choice(employees_data)\n",
    "    expense_type = random.choice(non_rent_expense_choices)\n",
    "    expense_amount = round(random.uniform(100, 1000), 2)\n",
    "    expense_date = fake.date_between(start_date='-5y', end_date=date(2024, 6, 30))\n",
    "    \n",
    "    if monthly_expenses[expense_date.strftime('%Y-%m')] + expense_amount > 10000:\n",
    "        continue\n",
    "    \n",
    "    expense_record = {\n",
    "        'office_name': office['office_name'],\n",
    "        'office_address': office['address'],\n",
    "        'office_city': office['city'],\n",
    "        'office_state': office['state'],\n",
    "        'office_zipcode': office['zipcode'],\n",
    "        'expense_date': expense_date,\n",
    "        'expense_type': expense_type,\n",
    "        'amount': expense_amount,\n",
    "        'description': '',\n",
    "        'approved_by_name': employee['name'],\n",
    "        'approved_by_email': employee['email'],\n",
    "        'approved_by_phone_number': employee['phone_number']\n",
    "    }\n",
    "    business_expenses_data.append(expense_record)\n",
    "    monthly_expenses[expense_date.strftime('%Y-%m')] += expense_amount\n",
    "\n",
    "# Save the data to a CSV file\n",
    "business_expenses_df = pd.DataFrame(business_expenses_data)\n",
    "business_expenses_csv = os.path.join(subdirectory, 'business_expenses.csv')\n",
    "business_expenses_df.to_csv(business_expenses_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Sample Data\n",
    "\n",
    "Load the generated CSV files to our database using psycopg2. The code will force all connections to the nyc_dream_homes database to be disconnected, remove the database from the instance, and then recreate the database and schema before proceeding with the import.\n",
    "\n",
    "Note: If psycopg2 is not installed, execute `pip install psycopg2-binary` in your terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration settings\n",
    "\n",
    "Change these values if needed for your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection parameters\n",
    "dbname = 'nyc_dream_homes'\n",
    "user = 'postgres'\n",
    "password = '123'\n",
    "host = 'localhost'\n",
    "port = '5432'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute a single SQL command\n",
    "def execute_sql_command(conn, command):\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(command)\n",
    "        conn.commit()\n",
    "\n",
    "# Function to drop all active connections to the database\n",
    "def drop_active_connections(conn, dbname):\n",
    "    drop_connections_query = f\"\"\"\n",
    "    SELECT pg_terminate_backend(pg_stat_activity.pid)\n",
    "    FROM pg_stat_activity\n",
    "    WHERE pg_stat_activity.datname = '{dbname}'\n",
    "      AND pid <> pg_backend_pid();\n",
    "    \"\"\"\n",
    "    execute_sql_command(conn, drop_connections_query)\n",
    "\n",
    "# Function to drop and recreate the database\n",
    "def recreate_database():\n",
    "    # Connect to the default database to drop and recreate the target database\n",
    "    conn = psycopg2.connect(\n",
    "        dbname='postgres',\n",
    "        user=user,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port\n",
    "    )\n",
    "    conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    drop_active_connections(conn, dbname)\n",
    "    execute_sql_command(conn, f'DROP DATABASE IF EXISTS {dbname};')\n",
    "    execute_sql_command(conn, f'CREATE DATABASE {dbname};')\n",
    "    conn.close()\n",
    "\n",
    "# Function to run schema creation SQL commands from a file\n",
    "def run_schema_creation(conn, schema_file_path):\n",
    "    with open(schema_file_path, 'r') as schema_file:\n",
    "        schema_sql = schema_file.read()\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(schema_sql)\n",
    "        conn.commit()\n",
    "\n",
    "# Connect to the recreated database\n",
    "def connect_to_database():\n",
    "    return psycopg2.connect(\n",
    "        dbname=dbname,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port\n",
    "    )\n",
    "\n",
    "# Function to map Pandas dtypes to PostgreSQL dtypes\n",
    "def map_dtype(dtype):\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        return 'INTEGER'\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return 'NUMERIC'\n",
    "    elif pd.api.types.is_bool_dtype(dtype):\n",
    "        return 'BOOLEAN'\n",
    "    elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "        return 'DATE'\n",
    "    else:\n",
    "        return 'TEXT'\n",
    "\n",
    "# Function to drop tables if they already exist\n",
    "def drop_table_if_exists(table_name, conn):\n",
    "    drop_table_query = f\"DROP TABLE IF EXISTS {table_name};\"\n",
    "    execute_sql_command(conn, drop_table_query)\n",
    "\n",
    "# Function to create tables based on CSV structure\n",
    "def create_table_from_csv(csv_path, table_name, conn, date_columns=[]):\n",
    "    drop_table_if_exists(table_name, conn)\n",
    "    df = pd.read_csv(csv_path, parse_dates=date_columns)\n",
    "    col_str = \", \".join([f\"{col} {map_dtype(df[col].dtype)}\" for col in df.columns])\n",
    "    create_table_query = f\"CREATE TABLE {table_name} ({col_str});\"\n",
    "    execute_sql_command(conn, create_table_query)\n",
    "\n",
    "# Function to load data from CSV into PostgreSQL\n",
    "def load_data_to_postgres(csv_path, table_name, conn):\n",
    "    with conn.cursor() as cur:\n",
    "        with open(csv_path, 'r') as f:\n",
    "            cur.copy_expert(f\"COPY {table_name} FROM STDIN WITH CSV HEADER\", f)\n",
    "        conn.commit()\n",
    "    return verify_table(conn, table_name)\n",
    "\n",
    "# Functions to verify if tables were loaded properly\n",
    "def verify_table(conn, table_name):\n",
    "    with conn.cursor() as cur:\n",
    "        check_table_query = f\"SELECT COUNT(*) FROM {table_name};\"\n",
    "        cur.execute(check_table_query)\n",
    "        count = cur.fetchone()[0]\n",
    "    return count\n",
    "\n",
    "def display_results(table_counts):\n",
    "    for table, count in table_counts.items():\n",
    "        print(f\"Table {table} has {count} records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild NYC Dream Homes\n",
    "\n",
    "Recreate the database from scratch removing all previous data, then import the temporary tables from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table tmp_clients has 1000 records.\n",
      "Table tmp_employees has 250 records.\n",
      "Table tmp_properties has 20000 records.\n",
      "Table tmp_events has 5000 records.\n",
      "Table tmp_offices has 10 records.\n",
      "Table tmp_transactions has 2000 records.\n",
      "Table tmp_property_reviews has 49859 records.\n",
      "Table tmp_property_images has 99328 records.\n",
      "Table tmp_client_interactions has 2532 records.\n",
      "Table tmp_business_expenses has 416 records.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "recreate_database()\n",
    "\n",
    "# Connect to the newly created database\n",
    "conn = connect_to_database()\n",
    "\n",
    "# Run schema creation from external SQL file\n",
    "schema_file_path = '../schema/schema_master.sql'\n",
    "run_schema_creation(conn, schema_file_path)\n",
    "\n",
    "# Create temporary tables\n",
    "create_table_from_csv(clients_csv, 'tmp_clients', conn, date_columns=['date_of_birth'])\n",
    "create_table_from_csv(employees_csv, 'tmp_employees', conn, date_columns=['date_of_birth', 'hire_date', 'termination_date'])\n",
    "create_table_from_csv(properties_csv, 'tmp_properties', conn, date_columns=['listing_date', 'agent_date_of_birth'])\n",
    "create_table_from_csv(events_csv, 'tmp_events', conn, date_columns=['date', 'client_date_of_birth', 'agent_date_of_birth'])\n",
    "create_table_from_csv(offices_csv, 'tmp_offices', conn, date_columns=[])\n",
    "create_table_from_csv(transactions_csv, 'tmp_transactions', conn, date_columns=['date', 'client_date_of_birth', 'agent_date_of_birth', 'contract_signed_date', 'closing_date'])\n",
    "create_table_from_csv(property_reviews_csv, 'tmp_property_reviews', conn, date_columns=['client_date_of_birth', 'review_date'])\n",
    "create_table_from_csv(property_images_csv, 'tmp_property_images', conn, date_columns=['uploaded_date'])\n",
    "create_table_from_csv(client_interactions_csv, 'tmp_client_interactions', conn, date_columns=['client_date_of_birth', 'agent_date_of_birth', 'interaction_date'])\n",
    "create_table_from_csv(business_expenses_csv, 'tmp_business_expenses', conn, date_columns=['expense_date'])\n",
    "\n",
    "table_counts = {\n",
    "    'tmp_clients': load_data_to_postgres(clients_csv, 'tmp_clients', conn),\n",
    "    'tmp_employees': load_data_to_postgres(employees_csv, 'tmp_employees', conn),\n",
    "    'tmp_properties': load_data_to_postgres(properties_csv, 'tmp_properties', conn),\n",
    "    'tmp_events': load_data_to_postgres(events_csv, 'tmp_events', conn),\n",
    "    'tmp_offices': load_data_to_postgres(offices_csv, 'tmp_offices', conn),\n",
    "    'tmp_transactions': load_data_to_postgres(transactions_csv, 'tmp_transactions', conn),\n",
    "    'tmp_property_reviews': load_data_to_postgres(property_reviews_csv, 'tmp_property_reviews', conn),\n",
    "    'tmp_property_images': load_data_to_postgres(property_images_csv, 'tmp_property_images', conn),\n",
    "    'tmp_client_interactions': load_data_to_postgres(client_interactions_csv, 'tmp_client_interactions', conn),\n",
    "    'tmp_business_expenses': load_data_to_postgres(business_expenses_csv, 'tmp_business_expenses', conn)\n",
    "}\n",
    "\n",
    "# Display the results\n",
    "display_results(table_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Schema via SQL\n",
    "\n",
    "The following code has been added to streamline the execution of the SQL used to transfer all of the temporary data to the database schema.\n",
    "\n",
    "The SQL is stored in `load_data.sql` and can be executed directly in PostgreSQL if preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to run schema creation SQL commands from a file\n",
    "def run_schema_load(conn, load_file_path):\n",
    "    with open(load_file_path, 'r') as load_file:\n",
    "        load_sql = load_file.read()\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(load_sql)\n",
    "        conn.commit()\n",
    "\n",
    "# Run final data load from external SQL file\n",
    "load_file_path = 'load_data.sql'\n",
    "run_schema_load(conn, load_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the temporary tables from the production database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table_name in table_counts.keys():\n",
    "    drop_table_if_exists(table_name, conn)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
