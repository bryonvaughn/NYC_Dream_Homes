{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "from custom_providers import NYCAddressProvider, NYCPersonProvider, RentalDescriptionProvider, PropertyAmenitiesProvider, ExtendedZipcodeProvider\n",
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the custom providers for more relevant sample data, and where the data files will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = Faker()\n",
    "\n",
    "# configure the custom providers for our fake data elements\n",
    "fake.add_provider(NYCAddressProvider)\n",
    "fake.add_provider(NYCPersonProvider)\n",
    "fake.add_provider(RentalDescriptionProvider)\n",
    "fake.add_provider(PropertyAmenitiesProvider)\n",
    "fake.add_provider(ExtendedZipcodeProvider)\n",
    "\n",
    "# Create the data subdirectory if it doesn't exist\n",
    "subdirectory = 'data'\n",
    "os.makedirs(subdirectory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 10 offices for the organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "offices_data = []\n",
    "\n",
    "for i in range(10):\n",
    "    address = fake.nyc_address()\n",
    "    office_record = {\n",
    "        'office_name': 'Dream Homes NYC Branch',\n",
    "        'address': address['address'],\n",
    "        'city': address['city'],\n",
    "        'state': address['state'],\n",
    "        'zipcode': address['zipcode'],\n",
    "        'neighborhood': address['neighborhood'],\n",
    "        'phone_number': fake.phone_number(),\n",
    "        'email': fake.company_email()\n",
    "    }\n",
    "    offices_data.append(office_record)\n",
    "\n",
    "offices_df = pd.DataFrame(offices_data)\n",
    "offices_csv = os.path.join(subdirectory, 'offices.csv')\n",
    "offices_df.to_csv(offices_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 50 denormalized employee records, make every other employee an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_data = []\n",
    "agents_data = []\n",
    "used_names = set()\n",
    "\n",
    "for i in range(50):\n",
    "    address = fake.nyc_address()\n",
    "    person = fake.nyc_person(used_names)\n",
    "    office = random.choice(offices_data)  # Select a random office for the employee\n",
    "    employee_record = {\n",
    "        'name': person['name'],\n",
    "        'email': person['email'],\n",
    "        'phone_number': person['phone_number'],\n",
    "        'date_of_birth': fake.date_of_birth(minimum_age=18, maximum_age=80),\n",
    "        'address': address['address'],\n",
    "        'city': address['city'],\n",
    "        'state': address['state'],\n",
    "        'zipcode': address['zipcode'],\n",
    "        'neighborhood': address['neighborhood'],\n",
    "        'office_phone_number': office['phone_number'],\n",
    "        'office_email': office['email'],\n",
    "        'employment_status': random.choice(['Active', 'Inactive', 'On Leave', 'Terminated']),\n",
    "        'sales_total': 0,\n",
    "        'hire_date': fake.date_between(start_date='-5y', end_date='today'),\n",
    "        'termination_date': fake.date_between(start_date='-5y', end_date='today') if random.choice([True, False]) else None,\n",
    "        'license_number': fake.bothify(text='???-####-####') if i % 2 == 0 else None,\n",
    "        'specialties': fake.sentence() if i % 2 == 0 else None,\n",
    "        'rating': round(random.uniform(1, 5), 2) if i % 2 == 0 else None\n",
    "    }\n",
    "    employees_data.append(employee_record.copy())\n",
    "\n",
    "    if i % 2 == 0:  # Half of the employees are also agents\n",
    "        agents_data.append(employee_record.copy())\n",
    "\n",
    "employees_df = pd.DataFrame(employees_data)\n",
    "employees_csv = os.path.join(subdirectory, 'employees.csv')\n",
    "employees_df.to_csv(employees_csv, index=False)\n",
    "\n",
    "agents_df = pd.DataFrame(agents_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 200 denormalized client records, and up to five interactions with agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static Property Types and Statuses\n",
    "client_types = ['Corporate', 'Individual', 'Non-Profit', 'Government', 'Small Business', 'VIP']\n",
    "interaction_types = ['Email', 'Phone Call', 'In-Person Meeting', 'Follow-Up', 'Inquiry']\n",
    "\n",
    "clients_data = []\n",
    "client_interactions_data = []\n",
    "used_names = set()\n",
    "\n",
    "for _ in range(200):\n",
    "    address = fake.nyc_address()\n",
    "    person = fake.nyc_person(used_names)\n",
    "    agent = random.choice(agents_data)  # Select a random agent for any interactions\n",
    "    client_record = {\n",
    "        'name': person['name'],\n",
    "        'email': person['email'],\n",
    "        'phone_number': person['phone_number'],\n",
    "        'date_of_birth': fake.date_of_birth(minimum_age=18, maximum_age=80),\n",
    "        'address': address['address'],\n",
    "        'city': address['city'],\n",
    "        'state': address['state'],\n",
    "        'zipcode': address['zipcode'],\n",
    "        'neighborhood': address['neighborhood'],\n",
    "        'preferred_contact_method': random.choice(['Email', 'Phone', 'Mail']),\n",
    "        'notes': fake.text(),\n",
    "        'client_type': random.choice(client_types)\n",
    "    }\n",
    "    clients_data.append(client_record)\n",
    "\n",
    "    num_interactions = random.randint(0, 5)\n",
    "    for _ in range(num_interactions):\n",
    "        interaction_record = {\n",
    "            'client_name': client_record['name'],\n",
    "            'client_email': client_record['email'],\n",
    "            'client_date_of_birth': client_record['date_of_birth'],\n",
    "            'agent_name': agent['name'],\n",
    "            'agent_email': agent['email'],\n",
    "            'agent_date_of_birth': agent['date_of_birth'],\n",
    "            'interaction_date': fake.date_between(start_date='-1y', end_date='today'),\n",
    "            'interaction_type': random.choice(interaction_types),\n",
    "            'notes': fake.text()\n",
    "        }\n",
    "        client_interactions_data.append(interaction_record)\n",
    "\n",
    "clients_df = pd.DataFrame(clients_data)\n",
    "clients_csv = os.path.join(subdirectory, 'clients.csv')\n",
    "clients_df.to_csv(clients_csv, index=False)\n",
    "\n",
    "client_interactions_df = pd.DataFrame(client_interactions_data)\n",
    "client_interactions_csv = os.path.join(subdirectory, 'client_interactions.csv')\n",
    "client_interactions_df.to_csv(client_interactions_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 2000 denormalized property records, randomize the property_type, property_status, and listing_agent assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static Property Types and Statuses\n",
    "property_types = ['Apartment', 'Townhouse', 'Condo', 'Villa', 'Studio']\n",
    "property_statuses = ['Pending', 'Reserved', 'Sold', 'Listed', 'Unavailable']\n",
    "\n",
    "properties_data = []\n",
    "property_reviews_data = []\n",
    "property_images_data = []\n",
    "\n",
    "for _ in range(2000):\n",
    "    address = fake.nyc_address()\n",
    "    agent = random.choice(agents_data)  # Select a random agent for the listing agent\n",
    "    property_record = {\n",
    "        'address': address['address'],\n",
    "        'city': address['city'],\n",
    "        'state': address['state'],\n",
    "        'zipcode': address['zipcode'],\n",
    "        'neighborhood': address['neighborhood'],\n",
    "        'type_name': random.choice(property_types),\n",
    "        'status_name': random.choice(property_statuses),\n",
    "        'price': round(random.uniform(100000, 1000000), 2),\n",
    "        'square_feet': random.randint(500, 5000),\n",
    "        'number_of_bedrooms': random.randint(1, 5),\n",
    "        'number_of_bathrooms': random.randint(1, 3),\n",
    "        'year_built': random.randint(1900, 2021),\n",
    "        'description': fake.rental_description(),\n",
    "        'amenities': fake.property_amenities(),\n",
    "        'listing_date': fake.date_between(start_date='-5y', end_date='today'),\n",
    "        'agent_name': agent['name'],\n",
    "        'agent_email': agent['email'],\n",
    "        'agent_phone': agent['phone_number'],\n",
    "        'agent_date_of_birth': agent['date_of_birth']\n",
    "    }\n",
    "    properties_data.append(property_record)\n",
    "\n",
    "    # Generate up to 5 property reviews\n",
    "    num_reviews = random.randint(0, 5)\n",
    "    for _ in range(num_reviews):\n",
    "        client = random.choice(clients_data)\n",
    "        review_record = {\n",
    "            'property_address': address['address'],\n",
    "            'property_city': address['city'],\n",
    "            'property_state': address['state'],\n",
    "            'property_zipcode': address['zipcode'],\n",
    "            'client_name': client['name'],\n",
    "            'client_email': client['email'],\n",
    "            'client_date_of_birth': client['date_of_birth'],\n",
    "            'review_date': fake.date_between(start_date='-1y', end_date='today'),\n",
    "            'rating': random.randint(1, 5),\n",
    "            'review_text': fake.text()\n",
    "        }\n",
    "        property_reviews_data.append(review_record)\n",
    "\n",
    "    # Generate up to 10 property images\n",
    "    num_images = random.randint(0, 10)\n",
    "    for _ in range(num_images):\n",
    "        image_record = {\n",
    "            'property_address': address['address'],\n",
    "            'property_city': address['city'],\n",
    "            'property_state': address['state'],\n",
    "            'property_zipcode': address['zipcode'],\n",
    "            'image_url': fake.image_url(),\n",
    "            'description': fake.text(),\n",
    "            'uploaded_date': fake.date_between(start_date='-1y', end_date='today')\n",
    "        }\n",
    "        property_images_data.append(image_record)\n",
    "\n",
    "properties_df = pd.DataFrame(properties_data)\n",
    "properties_csv = os.path.join(subdirectory, 'properties.csv')\n",
    "properties_df.to_csv(properties_csv, index=False)\n",
    "\n",
    "property_reviews_df = pd.DataFrame(property_reviews_data)\n",
    "property_reviews_csv = os.path.join(subdirectory, 'property_reviews.csv')\n",
    "property_reviews_df.to_csv(property_reviews_csv, index=False)\n",
    "\n",
    "property_images_df = pd.DataFrame(property_images_data)\n",
    "property_images_csv = os.path.join(subdirectory, 'property_images.csv')\n",
    "property_images_df.to_csv(property_images_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 200 denormalized event records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_types = [\n",
    "    'Personal Showing', 'Virtual Tour', 'Open House'\n",
    "]\n",
    "\n",
    "# Generate Events\n",
    "events_data = []\n",
    "for _ in range(200):\n",
    "    agent = random.choice(agents_data)          # Select a random agent for the event\n",
    "    property = random.choice(properties_data)   # Select a random property for the event\n",
    "    client = random.choice(clients_data)        # Select a random client for the event\n",
    "    event_record = {\n",
    "        'type_name': random.choice(event_types),\n",
    "        'date': fake.date_between(start_date='-1y', end_date='today'),\n",
    "        'client_attended': random.choice([True, False]),\n",
    "        'duration': random.randint(1, 2) * 60,\n",
    "        'client_name': client['name'],\n",
    "        'client_email': client['email'],\n",
    "        'client_date_of_birth': client['date_of_birth'],\n",
    "        'property_address': property['address'],\n",
    "        'property_city': property['city'],\n",
    "        'property_state': property['state'],\n",
    "        'property_zipcode': property['zipcode'],\n",
    "        'property_neighborhood': property['neighborhood'],\n",
    "        'agent_name': agent['name'],\n",
    "        'agent_email': agent['email'],\n",
    "        'agent_phone': agent['phone_number'],\n",
    "        'agent_date_of_birth': agent['date_of_birth']\n",
    "    }\n",
    "    events_data.append(event_record)\n",
    "\n",
    "events_df = pd.DataFrame(events_data)\n",
    "events_csv = os.path.join(subdirectory, 'events.csv')\n",
    "events_df.to_csv(events_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 500 denormalized transaction records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_types = ['Purchase', 'Sale', 'Rent']\n",
    "\n",
    "transactions_data = []\n",
    "for _ in range(500):\n",
    "    agent = random.choice(agents_data)\n",
    "    client = random.choice(clients_data)\n",
    "    property = random.choice(properties_data)\n",
    "    transaction_type = random.choice(transaction_types)\n",
    "    transaction_record = {\n",
    "        'property_address': property['address'],\n",
    "        'property_city': property['city'],\n",
    "        'property_state': property['state'],\n",
    "        'property_zipcode': property['zipcode'],\n",
    "        'property_neighborhood': property['neighborhood'],\n",
    "        'client_name': client['name'],\n",
    "        'client_email': client['email'],\n",
    "        'client_date_of_birth': client['date_of_birth'],\n",
    "        'agent_name': agent['name'],\n",
    "        'agent_email': agent['email'],\n",
    "        'agent_phone': agent['phone_number'],\n",
    "        'agent_date_of_birth': agent['date_of_birth'],\n",
    "        'transaction_type': transaction_type,\n",
    "        'price': round(random.uniform(100000, 1000000), 2),\n",
    "        'commission': round(random.uniform(5000, 50000), 2),\n",
    "        'date': fake.date_between(start_date='-1y', end_date='today'),\n",
    "        'contract_signed_date': fake.date_between(start_date='-1y', end_date='today'),\n",
    "        'closing_date': fake.date_between(start_date='-1y', end_date='today')\n",
    "    }\n",
    "    transactions_data.append(transaction_record)\n",
    "\n",
    "transactions_df = pd.DataFrame(transactions_data)\n",
    "transactions_csv = os.path.join(subdirectory, 'transactions.csv')\n",
    "transactions_df.to_csv(transactions_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 500 business expense records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "expense_types = ['Office Supplies', 'Utilities', 'Rent', 'Marketing', 'Travel', 'Maintenance', 'Technology', 'Training', 'Event', 'Miscellaneous']\n",
    "\n",
    "business_expenses_data = []\n",
    "\n",
    "for _ in range(500):\n",
    "    office = random.choice(offices_data)\n",
    "    employee = random.choice(employees_data)\n",
    "    expense_record = {\n",
    "        'office_name': office['office_name'],\n",
    "        'office_address': office['address'],\n",
    "        'office_city': office['city'],\n",
    "        'office_state': office['state'],\n",
    "        'office_zipcode': office['zipcode'],\n",
    "        'expense_date': fake.date_between(start_date='-1y', end_date='today'),\n",
    "        'expense_type': random.choice(expense_types),\n",
    "        'amount': round(random.uniform(100, 5000), 2),\n",
    "        'description': fake.text(),\n",
    "        'approved_by_name': employee['name'],\n",
    "        'approved_by_email': employee['email'],\n",
    "        'approved_by_phone_number': employee['phone_number']\n",
    "    }\n",
    "    business_expenses_data.append(expense_record)\n",
    "\n",
    "business_expenses_df = pd.DataFrame(business_expenses_data)\n",
    "business_expenses_csv = os.path.join(subdirectory, 'business_expenses.csv')\n",
    "business_expenses_df.to_csv(business_expenses_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Sample Data\n",
    "\n",
    "Load the generated CSV files to our database using psycopg2. The code will force all connections to the nyc_dream_homes database to be disconnected, remove the database from the instance, and then recreate the database and schema before proceeding with the import.\n",
    "\n",
    "Note: If psycopg2 is not installed, execute `pip install psycopg2-binary` in your terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration settings\n",
    "\n",
    "Change these values if needed for your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection parameters\n",
    "dbname = 'nyc_dream_homes'\n",
    "user = 'postgres'\n",
    "password = '123'\n",
    "host = 'localhost'\n",
    "port = '5432'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute a single SQL command\n",
    "def execute_sql_command(conn, command):\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(command)\n",
    "        conn.commit()\n",
    "\n",
    "# Function to drop all active connections to the database\n",
    "def drop_active_connections(conn, dbname):\n",
    "    drop_connections_query = f\"\"\"\n",
    "    SELECT pg_terminate_backend(pg_stat_activity.pid)\n",
    "    FROM pg_stat_activity\n",
    "    WHERE pg_stat_activity.datname = '{dbname}'\n",
    "      AND pid <> pg_backend_pid();\n",
    "    \"\"\"\n",
    "    execute_sql_command(conn, drop_connections_query)\n",
    "\n",
    "# Function to drop and recreate the database\n",
    "def recreate_database():\n",
    "    # Connect to the default database to drop and recreate the target database\n",
    "    conn = psycopg2.connect(\n",
    "        dbname='postgres',\n",
    "        user=user,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port\n",
    "    )\n",
    "    conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    drop_active_connections(conn, dbname)\n",
    "    execute_sql_command(conn, f'DROP DATABASE IF EXISTS {dbname};')\n",
    "    execute_sql_command(conn, f'CREATE DATABASE {dbname};')\n",
    "    conn.close()\n",
    "\n",
    "# Function to run schema creation SQL commands from a file\n",
    "def run_schema_creation(conn, schema_file_path):\n",
    "    with open(schema_file_path, 'r') as schema_file:\n",
    "        schema_sql = schema_file.read()\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(schema_sql)\n",
    "        conn.commit()\n",
    "\n",
    "# Connect to the recreated database\n",
    "def connect_to_database():\n",
    "    return psycopg2.connect(\n",
    "        dbname=dbname,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port\n",
    "    )\n",
    "\n",
    "# Function to map Pandas dtypes to PostgreSQL dtypes\n",
    "def map_dtype(dtype):\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        return 'INTEGER'\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return 'NUMERIC'\n",
    "    elif pd.api.types.is_bool_dtype(dtype):\n",
    "        return 'BOOLEAN'\n",
    "    elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "        return 'DATE'\n",
    "    else:\n",
    "        return 'TEXT'\n",
    "\n",
    "# Function to drop tables if they already exist\n",
    "def drop_table_if_exists(table_name, conn):\n",
    "    drop_table_query = f\"DROP TABLE IF EXISTS {table_name};\"\n",
    "    execute_sql_command(conn, drop_table_query)\n",
    "\n",
    "# Function to create tables based on CSV structure\n",
    "def create_table_from_csv(csv_path, table_name, conn, date_columns=[]):\n",
    "    drop_table_if_exists(table_name, conn)\n",
    "    df = pd.read_csv(csv_path, parse_dates=date_columns)\n",
    "    col_str = \", \".join([f\"{col} {map_dtype(df[col].dtype)}\" for col in df.columns])\n",
    "    create_table_query = f\"CREATE TABLE {table_name} ({col_str});\"\n",
    "    execute_sql_command(conn, create_table_query)\n",
    "\n",
    "# Function to load data from CSV into PostgreSQL\n",
    "def load_data_to_postgres(csv_path, table_name, conn):\n",
    "    with conn.cursor() as cur:\n",
    "        with open(csv_path, 'r') as f:\n",
    "            cur.copy_expert(f\"COPY {table_name} FROM STDIN WITH CSV HEADER\", f)\n",
    "        conn.commit()\n",
    "    return verify_table(conn, table_name)\n",
    "\n",
    "# Functions to verify if tables were loaded properly\n",
    "def verify_table(conn, table_name):\n",
    "    with conn.cursor() as cur:\n",
    "        check_table_query = f\"SELECT COUNT(*) FROM {table_name};\"\n",
    "        cur.execute(check_table_query)\n",
    "        count = cur.fetchone()[0]\n",
    "    return count\n",
    "\n",
    "def display_results(table_counts):\n",
    "    for table, count in table_counts.items():\n",
    "        print(f\"Table {table} has {count} records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild NYC Dream Homes\n",
    "\n",
    "Recreate the database from scratch removing all previous data, then import the temporary tables from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table tmp_clients has 200 records.\n",
      "Table tmp_employees has 50 records.\n",
      "Table tmp_properties has 2000 records.\n",
      "Table tmp_events has 200 records.\n",
      "Table tmp_offices has 10 records.\n",
      "Table tmp_transactions has 500 records.\n",
      "Table tmp_property_reviews has 4974 records.\n",
      "Table tmp_property_images has 10138 records.\n",
      "Table tmp_client_interactions has 505 records.\n",
      "Table tmp_business_expenses has 500 records.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "recreate_database()\n",
    "\n",
    "# Connect to the newly created database\n",
    "conn = connect_to_database()\n",
    "\n",
    "# Run schema creation from external SQL file\n",
    "schema_file_path = '../schema/schema_master.sql'\n",
    "run_schema_creation(conn, schema_file_path)\n",
    "\n",
    "# Create temporary tables\n",
    "create_table_from_csv(clients_csv, 'tmp_clients', conn, date_columns=['date_of_birth'])\n",
    "create_table_from_csv(employees_csv, 'tmp_employees', conn, date_columns=['date_of_birth', 'hire_date', 'termination_date'])\n",
    "create_table_from_csv(properties_csv, 'tmp_properties', conn, date_columns=['listing_date', 'agent_date_of_birth'])\n",
    "create_table_from_csv(events_csv, 'tmp_events', conn, date_columns=['date', 'client_date_of_birth', 'agent_date_of_birth'])\n",
    "create_table_from_csv(offices_csv, 'tmp_offices', conn, date_columns=[])\n",
    "create_table_from_csv(transactions_csv, 'tmp_transactions', conn, date_columns=['date', 'client_date_of_birth', 'agent_date_of_birth', 'contract_signed_date', 'closing_date'])\n",
    "create_table_from_csv(property_reviews_csv, 'tmp_property_reviews', conn, date_columns=['client_date_of_birth', 'review_date'])\n",
    "create_table_from_csv(property_images_csv, 'tmp_property_images', conn, date_columns=['uploaded_date'])\n",
    "create_table_from_csv(client_interactions_csv, 'tmp_client_interactions', conn, date_columns=['client_date_of_birth', 'agent_date_of_birth', 'interaction_date'])\n",
    "create_table_from_csv(business_expenses_csv, 'tmp_business_expenses', conn, date_columns=['expense_date'])\n",
    "\n",
    "table_counts = {\n",
    "    'tmp_clients': load_data_to_postgres(clients_csv, 'tmp_clients', conn),\n",
    "    'tmp_employees': load_data_to_postgres(employees_csv, 'tmp_employees', conn),\n",
    "    'tmp_properties': load_data_to_postgres(properties_csv, 'tmp_properties', conn),\n",
    "    'tmp_events': load_data_to_postgres(events_csv, 'tmp_events', conn),\n",
    "    'tmp_offices': load_data_to_postgres(offices_csv, 'tmp_offices', conn),\n",
    "    'tmp_transactions': load_data_to_postgres(transactions_csv, 'tmp_transactions', conn),\n",
    "    'tmp_property_reviews': load_data_to_postgres(property_reviews_csv, 'tmp_property_reviews', conn),\n",
    "    'tmp_property_images': load_data_to_postgres(property_images_csv, 'tmp_property_images', conn),\n",
    "    'tmp_client_interactions': load_data_to_postgres(client_interactions_csv, 'tmp_client_interactions', conn),\n",
    "    'tmp_business_expenses': load_data_to_postgres(business_expenses_csv, 'tmp_business_expenses', conn)\n",
    "}\n",
    "\n",
    "# Display the results\n",
    "display_results(table_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Schema via SQL\n",
    "\n",
    "The following code has been added to streamline the execution of the SQL used to transfer all of the temporary data to the database schema.\n",
    "\n",
    "The SQL is stored in `load_data.sql` and can be executed directly in PostgreSQL if preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to run schema creation SQL commands from a file\n",
    "def run_schema_load(conn, load_file_path):\n",
    "    with open(load_file_path, 'r') as load_file:\n",
    "        load_sql = load_file.read()\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(load_sql)\n",
    "        conn.commit()\n",
    "\n",
    "# Run final data load from external SQL file\n",
    "load_file_path = 'load_data.sql'\n",
    "run_schema_load(conn, load_file_path)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
